% This is based on the LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% See http://www.springer.com/computer/lncs/lncs+authors?SGWID=0-40209-0-0-0
% for the full guidelines.

\documentclass{llncs}

\hyphenation{do-cu-ment do-cu-ments va-rious}

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{multirow}
\usepackage[usenames, dvipsnames]{xcolor}
\definecolor{igual}{rgb}{0.21, 0.11, 0} 
\usepackage{hyperref}
\definecolor{dark-blue}{rgb}{0.0,0.0,0.1}
\definecolor{dark-green}{rgb}{0.0,0.1,0.0}
\definecolor{dark-red}{rgb}{0.1,0.0,0.0}
\hypersetup{
    colorlinks, linkcolor={dark-red},
    citecolor={dark-green}, urlcolor={dark-blue},
    pdftitle={Machine Translation vs. Multilingual Approaches for Entity Linking},    % title
    pdfauthor={Henry Rosales-Méndez, Aidan Hogan and Barbara Poblete},     % author
    pdfsubject={ISWC 2018},   % subject of the document
    pdfkeywords={multilingual;} {entity linking;} {information extraction;}, % list of keywords
}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

%para el simbolo de chequeado
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%
\newcommand{\xmark}{}%

\usepackage{moreverb}% usar verbatim + box
\usepackage{breqn}

\usepackage{booktabs}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{soul} %underline
\newcommand{\bl}[1]{\textbf{#1}}

\begin{document}

\title{Machine Translation vs. Multilingual Approaches for Entity Linking}
%
%
\author{Henry Rosales-M\'endez, Aidan Hogan and Barbara Poblete}
%
\authorrunning{Rosales-Méndez et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
%\institute{Center for Semantic Web Research, DCC, University of Chile \\
%\texttt{\{hrosales,bpoblete,ahogan\}@dcc.uchile.cl}}
\institute{IMFD Chile \& Department of Computer Science, University of Chile 
%	\\\texttt{\{hrosales,ahogan,bpoblete\}@dcc.uchile.cl}
}

%-----------------------------------------
\maketitle              % typeset the title of the contribution
\begin{abstract}
Entity Linking (EL) associates the entities mentioned in a given input text with their corresponding knowledge-base (KB) entries. A recent EL trend is towards multilingual approaches. However, one may ask: are multilingual EL approaches necessary with recent advancements in machine translation? Could we not simply focus on supporting one language in the EL system and translate the input text to that language? We present experiments along these lines comparing multilingual EL systems with their results over machine translated text.
\end{abstract}


%---------------------
\section{Introduction}
\label{sec:intro}

Entity Linking (EL) associates the entities mentioned in a given input text with their corresponding knowledge-base (KB) identifiers; e.g., taking Wikidata as a target KB, for the input text ``\textit{Michael Jackson was born in Gary, Indiana}'', we can link \textit{Michael Jackson} with the Wikidata identifier \url{wd:Q2831}. However, multiple KB entities may have the same label; e.g., \url{wd:Q167877}, \url{wd:Q6831554}, and \url{wd:Q3856193} are all identifiers for people called \textit{Michael Jackson} in Wikidata. On the other hand, the same entity can be mentioned multiple ways, e.g., ``\textit{Michael J. Jackson}'', ``\textit{Jackson}'', ``\textit{King of Pop}'', etc., can refer to \url{wd:Q2831}.

Another practical challenge is being able to cope with input texts from various languages. While many EL approaches have been proposed down through the years, only recently have multilingual EL approaches -- configurable for various input languages -- become more popular (e.g.,~\cite{ferragina2010tagme,daiber2013improving,Babelfy-moro2014entity,freme-ner2016}). 
%In order to support multiple languages, an EL approach has three main options: (1) employ language-agnostic components that can work with respect to any KB that has labels in the required language~\cite{KIM-popov2004kim,daiber2013improving}; however, this approach precludes the possibility of using many linguistic components -- such as Part-Of-Speech (POS) taggers, dependency parsers, etc. -- that are tied to a language but can help improve EL quality; (2) include language-specific components that may improve EL quality, but for a variety of languages, such as POS taggers for various languages~\cite{Babelfy-moro2014entity}; (3) focus on supporting one language, but employ external machine translation approaches to translate the input text to the given language~\cite{Monahan2011,Cassidy2012}.
%
 Despite this trend, there are few studies evaluating multilingual EL. Hence, in our paper accepted for the Resource Track at ISWC~\cite{ourISWC}, we propose a multilingual EL benchmark and use it to perform experiments in order to study the behaviour of state-of-the-art multilingual approaches for five languages: English, French, German, Italian, and Spanish. We call our dataset VoxEL; a particular design goal of the dataset is to have (insofar as possible) the same text in different languages, and in particular, the same annotations per sentence across languages. Thus performance across languages -- not just systems -- can be compared directly. We also compared the results of multilingual EL systems with what would be possible using a state-of-the-art machine translation approach (Google Translate) to translate the text to English (the primary language supported by most tools). We refer the reader to our Resource Track paper~\cite{ourISWC} for more details.

In this poster, we present some additional results omitted from the full paper for reasons of space. More specifically, the poster focuses on the question of how an \textit{a priori} machine translation process compares with multilingual EL approaches, contributing novel results using VoxEL to evaluate EL performance using machine translation of the input to languages other than English. More generally, in the poster session, we would like to discuss with interested attendees the interplay between multilingual EL and machine translation.


%-----------------------------------------
\section{Evaluating Multilingual Entity Linking Approaches}

%For a given text input with a set of entities mentioned $M$, and a KB with a set of identifiers $E$ and associated metadata, the EL task involves associating (or linking) each $m\in{}M$ with a corresponding entity $e\in{}E$. This EL process is commonly divided into two main phases: Entity Recognition (ER) and Entity Disambiguation (ED). The first one has the goal of identifying the entities mentions in the text. These identified entity mentions are passed to the second phase where each of them is disambiguated and linked with a KB identifier.
 
A multilingual EL system is characterised by being configurable for multiple input languages. In this work, we evaluate four multilingual EL systems with public APIs, namely \textit{Babelfy}~\cite{Babelfy-moro2014entity}, \textit{DBpedia Spotlight}~\cite{daiber2013improving}, \textit{FREME}~\cite{freme-ner2016} and \textit{TagME}~\cite{ferragina2010tagme}. For reasons of space, we refer to our previous work~\cite{ourLD4ID2017,ourISWC} for further details on these systems and other multilingual EL systems proposed in the literature.

%Details of the evaluated systems are presented in Table \ref{tab:multilingual_approaches}, where we present the year of publication, and the languages for which languages are evaluated in the publication (we note that the system may support more languages than evaluated). We observe that all systems support English, and that other languages evaluated are European in origin.


%\begin{table}[t!]
%	\centering
%	\caption{Overview of multilingual EL approaches evaluated in our experiments.}
%	\label{tab:multilingual_approaches}
%	\setlength{\tabcolsep}{3pt}
%	\begin{tabular}{lcccccr}
%		\toprule
%		\textbf{Name} &  \textbf{Year} & \textbf{Evaluated Languages} & \textbf{API}\\ \midrule
%		
%%		KIM \cite{KIM-popov2004kim} & 2004 &English, French, Spanish&\xmark&\cmark\\\midrule
%        
%        TagME \cite{ferragina2010tagme} & 2010 & English, German, Dutch & \cmark\\\midrule
%		
%%		SDA \cite{SDA-charton2011automatic}  & 2011 &English, French&\xmark&\xmark\\\midrule
%%        
%%        \textcolor{blue}{Monahan et al.}~\cite{Monahan2011} & 2011 & English, Chinese&\cmark & \xmark\\\midrule 
%%		
%%		ualberta \cite{guo2012ualberta}& 2012 &English, Chinese&\xmark&\xmark\\\midrule
%%		
%%		HITS \cite{fahrni2012hits} & 2012 & English, Spanish, Chinese&\xmark&\xmark\\\midrule
%%		
%%		THD \cite{THD-dojchinovski2012recognizing}  & 2012 &English, German, Dutch&\xmark&\cmark\\\midrule 
%%        
%%        \textcolor{blue}{Cassidy et al.}~\cite{Cassidy2012} & 2012  &English, Chinese&\cmark \xmark\\\midrule 
%		
%		\textit{DBpedia Spotlight}~\cite{daiber2013improving} & 2013  &English, Italian, Russian, Dutch, French,& \cmark\\
%		& &German, Spanish, Hungarian, Danish&&\\\midrule
%%		
%%		Wang-Tang \cite{wang2013boosting} & 2013 & English, Chinese&\xmark&\xmark\\\midrule
%%		
%%		MAG \cite{mag2017}& 2014 & English, German, Spanish&\xmark&\cmark\\
%%		& &French, Italian, Japanese,&&&\\
%%		& &Dutch&&&\\\midrule
%%		
%		\textit{Babelfy} \cite{Babelfy-moro2014entity}& 2014 &English, Spanish, French, German, Italian & \cmark\\\midrule
%%		
%%		WikiME \cite{Cross-Lingual-Wikifier-tsai2016cross} & 2016 & English, Spanish, French,&\xmark&\xmark\\
%%		& &Italian, Chinese, German,&&&\\
%%		& &Thai, Arabic, Turkish,&&&\\
%%		& &Tamil, Tagalog, Urdu, Hebrew&&&\\\midrule		
%%        
%        \textit{FREME}~\cite{freme-ner2016}&2016&English, German&\cmark\\
%		%& &French, Spanish, Russian&&&\\\midrule
%%		
%%		FEL~\cite{FEL-pappu2017lightweight}& 2017 &English, Spanish, Chinese&\xmark&\xmark\\
%%
%%		%&2016&IDIOMAS&\cmark&\cmark&\cmark\\
%		%& & French, Dutch &&&\\
%		\bottomrule
%	\end{tabular}
%\end{table}

%Instead of adding complexity to monolingual models in order to deal with a multilingual environment, others authors take advantage of machine translation to deal with several languages. For instance, Monahan et al.~\cite{Monahan2011} use Bing API\footnote{\url{http://www.microsofttranslator.com/dev/}; June 1st, 2018.} to translate the input text from Chinese to English, while Cassidy et al.~\cite{Cassidy2012} apply EL over Chinese using coreference resolution, various name translation techniques over the entity mentions to English, and machine translation to English. The use of machine translation brings advantages. In this way, monolingual systems deal with all the languages supported by the translator tool, which usually include the most popular languages in the world. Another advantage is that it is not necessary to deal with transliterations or cross-language links in cases when the target KB is available in a different language than the input text. 

%\section{VoxEL: A Multilingual EL Benchmark}


Evaluating multilingual EL systems requires benchmark datasets with texts in various languages. To further compare the quality of EL results across languages -- not just systems -- we need (insofar as possible) the same text and annotations in the different languages. Only a few such datasets have been proposed: TAC KBP\footnote{\url{https://tac.nist.gov/2017/KBP/}; June 1st, 2018.}, SemEval\footnote{\url{http://alt.qcri.org/semeval2018/}; June 1st, 2018.}, and MEANTIME~\cite{meantime16}. However, MEANTIME~\cite{meantime16} is the only publicly available dataset; SemEval is published by a third-party whereas the TAC KBP dataset we could not acquire. Furthermore, we found that these datasets exhibit differences in their annotations for different languages. For a more detailed explanation of multilingual benchmark datasets see~\cite{ourISWC} and for results comparing various EL systems over the SemEval dataset, see~\cite{ourLD4ID2017}.

To support multilingual EL evaluation, in~\cite{ourISWC} we proposed VoxEL: a curated text extracted from the multilingual VoxEurop news site\footnote{\url{http://www.voxeurop.eu}; June 1st, 2018.} and manually annotated for EL benchmarking. This dataset contains 15 documents for each of the five supported languages: Germany, English, Spanish, French and Italian. To support comparison across languages, VoxEL was edited to ensure the same annotations per sentence across languages, normalising variances across languages. Given a lack of consensus on the definition of ``entity'', VoxEL features two annotated version of the documents for each language: one \textit{strict} that includes entities referring to people, places and organisations, and one \textit{relaxed} that includes links to all unambiguous pages of Wikipedia. Per language, VoxEL contains 204 and 674 annotations in the strict and relaxed version respectively.

%-----------------------------------------
\section{Experiments}

We conduct experiments using VoxEL to compare the behaviour of the four aforementioned multilingual EL systems for the five different languages offered by the dataset: German (DE), English (EN), Spanish (ES), French (FR) and Italian (IT). All systems were configured with their default parameters, except Babelfy, which allows to select a more strict or more relaxed notion of entity; we study the performance of both, denoted henceforth as Babelfy$_S$ and Babelfy$_R$ respectively. Aside from testing EL over the text in its native language, we also include results for EL applying machine translation -- namely Google Translate\footnote{\url{https://translate.google.com}; June 1st, 2018.} -- from each language of VoxEL to the other four languages; the purpose of this approach is to simulate an EL approach supporting one language and see if EL performs competitively when input text is translated from other languages. 

The results are given in Table~\ref{tab:expriments}, where we present the $F_1$-measure for various configurations. On the left we present the system and language configured. At the top of the table we present the Relaxed and Strict versions of the dataset, where for each version, we present the language of the input text, which is machine translated to the configured language; for example, row $\rightarrow \textit{ES}$, column \textit{DE $\rightarrow$}, gives the result for a German input text translated to Spanish (\textit{DE $\rightarrow$ ES}) and processed by the given EL systems configured for Spanish. Where input and translated languages coincide, we use the input text directly (such results are indicated with boxes). The best result per column for each dataset version and system is presented in bold. TagME supports English and German only.

\begin{table}[t!]
	\centering
	\caption{Comparison of EL systems for native and translated texts (F$_1$ measure)}
	\label{tab:expriments}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{@{}lcccccccccccc@{}}
			%\begin{tabular}{@{}lllllllllll@{}}
			\toprule
			&& \multicolumn{5}{c}{Relaxed} & \multicolumn{5}{c}{Strict}\\ \midrule
			&&DE $\rightarrow$&EN $\rightarrow$&ES $\rightarrow$&FR $\rightarrow$&IT $\rightarrow$&DE $\rightarrow$&EN $\rightarrow$&ES $\rightarrow$&FR $\rightarrow$&IT $\rightarrow$\\
			\cmidrule(lr){3-7}\cmidrule(lr){8-12}
			                &$\rightarrow DE$&~\fbox{0.523}~&~0.498~       &~0.495~           &~0.492~       &~0.490~       &~\fbox{0.344}~&~0.342~       &~0.365~       &~ 0.369~      &~{\bf 0.367}~\\
			                &$\rightarrow EN$&~0.508~       &~\fbox{0.545}~&~0.515~           &~0.506~       &~0.502~       &~0.299~       &~\fbox{0.319}~&~0.299~       &~0.315~       &~0.301~\\
            Babelfy$_R$~~~~~&$\rightarrow ES$&~{\bf 0.525}~ &~{\bf 0.558}~ &~\fbox{\bf 0.541}~&~{\bf 0.552}~ &~{\bf 0.548}~ &~0.344~       &~0.356~       &~\fbox{0.362}~&~0.357~       &~0.348~\\
                            &$\rightarrow FR$&~0.493~       &~0.485~       &~0.502~           &~\fbox{0.493}~&~0.493~       &~0.332~       &~0.331~       &~0.342~       &~\fbox{0.309}~&~0.341~\\
                            &$\rightarrow IT$&~0.513~       &~0.527~       &~0.512~           &~0.533~       &~\fbox{0.504}~&~{\bf 0.366}~ &~{\bf 0.379}~ &~{\bf 0.377}~ &~{\bf 0.378}~ &~\fbox{0.365}~
                            \\\midrule
                            &$\rightarrow DE$&~\fbox{0.279}~&~0.271~       &~0.275~           &~0.285~       &~0.273~       &~\fbox{0.572}~&~0.584~       &~0.589~       &~0.606~       &~0.588~\\
			                &$\rightarrow EN$&~0.312~       &~\fbox{0.308}~&~0.309~           &~0.323~       &~0.304~       &~0.518~       &~\fbox{0.567}~&~0.523~       &~0.559~       &~0.533~\\
            Babelfy$_S$~~~~~&$\rightarrow ES$&~{\bf 0.318}~ &~{\bf 0.327}~ &~\fbox{\bf 0.325}~&~{\bf 0.334}~ &~{\bf 0.336}~ &~0.577~       &~0.607~       &~\fbox{0.611}~&~0.610~       &~0.590~\\
                            &$\rightarrow FR$&~0.301~       &~0.299~       &~ 0.312~          &~\fbox{0.290}~&~0.310~       &~0.574~       &~0.601~       &~0.608~       &~\fbox{0.583}~&~0.606~\\
                            &$\rightarrow IT$&~0.306~       &~0.319~       &~0.318~           &~0.321~       &~\fbox{0.311}~&~{\bf 0.604}~ &~{\bf 0.634}~ &~{\bf 0.640}~ &~{\bf 0.638}~ &~\fbox{\bf 0.616}~
                            \\\midrule
                            &$\rightarrow DE$&~\fbox{0.400}~&~0.139~       &~0.177~           &~0.155~       &~0.166~       &~\fbox{0.510}~&~0.220~       &~0.292~       &~0.248~       &~0.280~\\
			                &$\rightarrow EN$&~{\bf 0.442} ~&~\fbox{\bf 0.466}~&~{\bf 0.454}~ &~{\bf 0.465}~ &~{\bf0.450}~  &~{\bf 0.697}~ &~\fbox{\bf 0.707}~&~{\bf 0.695}~&~{\bf 0.722}~&~{\bf 0.730}~\\
            DBpedia Spotlight&$\rightarrow ES$&~0.159~      &~0.121~       &~\fbox{0.373}~    &~0.130~       &~0.199~       &~0.292~       &~0.209~       &~\fbox{0.513}~&~0.234~       &~0.350~\\
                            &$\rightarrow FR$&~0.176~       &~0.177~       &~0.181~           &~\fbox{0.314}~&~0.180~       &~0.245~       &~0.252~       &~0.252~       &~\fbox{0.464}~&~0.255~\\
                            &$\rightarrow IT$&~0.184~       &~0.163~       &~0.221~           &~0.158~       &~\fbox{0.382}~&~0.272~       &~0.219~       &~0.335~       &~0.223~       &~\fbox{0.601}~
                            \\\midrule
                            &$\rightarrow DE$&~\fbox{0.282}~&~0.072~       &~0.132~           &~0.114~       &~0.160~       &~\fbox{0.483}~&~0.154~       &~0.240~       &~0.179~       &~0.261~\\
			                &$\rightarrow EN$&~{\bf 0.401}~ &~\fbox{\bf 0.407}~&~{\bf 0.402}~ &~{\bf 0.397}~ &~{\bf 0.406}~ &~{\bf 0.700}~ &~\fbox{\bf 0.708}~&~{\bf 0.715}~&~{\bf 0.694}~&~0.713~\\
            FREME    ~~~~~~ &$\rightarrow ES$&~0.174~       &~0.117~       &~\fbox{0.302}~    &~0.147~       &~0.232~       &~0.319~       &~0.231~       &~\fbox{0.583}~&~0.269~       &~0.417~\\
                            &$\rightarrow FR$&~0.167~       &~0.143~       &~0.169~           &~\fbox{0.268}~&~0.214~       &~0.287~       &~0.278~       &~0.314~       &~\fbox{0.483}~&~0.322~\\
                            &$\rightarrow IT$&~0.164~       &~0.127~       &~0.205~           &~0.136~       &~\fbox{0.373}~&~0.321~       &~0.253~       &~0.413~       &~0.256~       &~\fbox{\bf 0.726}~
                            \\\midrule
           \multirow{2}{*}{TagME}
                            &$\rightarrow DE$&~\fbox{0.414}~&~0.100~       &~0.127~           &~0.119~       &~0.124~       &~\fbox{0.272}~&~0.122~       &0.153         &~0.137~       &~0.152~\\
			                &$\rightarrow EN$&~{\bf 0.432}~ &~\fbox{\bf 0.462}~&~{\bf 0.450}~ &~{\bf 0.442}~ &~{\bf 0.440}~ &~{\bf 0.331}~ &~\fbox{\bf 0.327}~&~{\bf 0.334}~&~{\bf 0.321}~&~{\bf 0.336}~\\\bottomrule 
		\end{tabular}%
	}
\end{table}


\section{Discussion}

In Table~\ref{tab:expriments}, we see that DBpedia Spotlight, FREME and TagMe often perform markedly better when the input text is either in English, or translated to English; the one exception to this trend is that FREME performs slightly better over the untranslated Italian text in the Strict version of the dataset than over the translated English text. On the other hand, Babelfy generally performs best for (translated) Spanish texts in the Relaxed version, and (translated) Italian texts in the Strict version, though performance across languages is more balanced in general than for the former systems. These results suggest that prior machine translation makes little difference in the case of Babelfy, but markedly improves the performance of other systems when dealing with non-English texts; the reasons for this may include the quality of language-specific components, the richness of KB information available for a particular language, etc. 

It is important to highlight in such cases that the output of the EL process after translation is still in the translated language; e.g., if we process text in French by translating it to English and performing EL configured for English, we may get better results, but the output text is in English, not French. But we put forward that given (1) a high(er) quality annotation in the translated English text, (2) a sentence-to-sentence correspondence between the French and translated English text, and (3) cross-language links provided by KBs; it would not be difficult to ``transfer'' the annotations back to the original French text. 

In any case, these results raise the question of what role machine translation should play for EL, and indeed, in what circumstances it makes sense to develop multilingual EL systems, and in what circumstances it makes sense to develop monolingual EL systems with \textit{a priori} translation. 

{\footnotesize
\paragraph{Acknowledgements} Henry Rosales-M\'endez was supported by CONICYT-PCHA/Doc\-torado Nacional/2016-21160017. The work was also supported by the Millennium Institute for Foundational Research on Data (IMFD) and by Fondecyt Grant No.\ 1181896.
}


\bibliographystyle{splncs}
\begin{thebibliography}{99}

\bibitem{daiber2013improving}
Daiber, J., Jakob, M., Hokamp, C., Mendes, P.N.: Improving efficiency and accuracy in multilingual entity extraction. In: I-SEMANTICS, ACM (2013) 121--124

\bibitem{ferragina2010tagme}
Ferragina, P., Scaiella, U.: Tagme: on-the-fly annotation of short text fragments (by Wikipedia entities). In: CIKM, ACM (2010) 1625--1628

\bibitem{meantime16} %11
Minard, A. L., et al. MEANTIME, the NewsReader multilingual event and time corpus. LREC-ELRA (2016)
%Minard, A. L., Speranza, M., Urizar, R., Altuna, B., van Erp, M. G. J., Schoen, A. M., van Son, C. M. MEANTIME, the NewsReader multilingual event and %time corpus. LREC-ELRA (2016)

\bibitem{Babelfy-moro2014entity}
Moro, A., Raganato, A., Navigli, R.: Entity linking meets word sense disambiguation: a unified approach. Trans. of the ACL \textbf{2} (2014) 231--244

\bibitem{ourISWC}
Rosales-Méndez, H., Hogan A., Poblete B. VoxEL: A Benchmark Dataset for Multilingual Entity Linking. In ISWC (2018) (to appear)

\bibitem{ourLD4ID2017}
Rosales-Méndez, H., Poblete, B., and Hogan, A. Multilingual Entity Linking: Comparing English and Spanish. In LD4IE@ISWC (2017)

\bibitem{freme-ner2016}
Sasaki, F., Dojchinovski, M., Nehring, J. Chainable and Extendable Knowledge Integration Web Services. In ISWC, (2016) 89--101

\end{thebibliography}

\end{document}

