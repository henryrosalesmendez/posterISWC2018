% This is based on the LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% See http://www.springer.com/computer/lncs/lncs+authors?SGWID=0-40209-0-0-0
% for the full guidelines.
%
%
%     To submit at Alberto Mendelzon Workshop 2018
%
%
\documentclass{llncs}

\hyphenation{do-cu-ment do-cu-ments}

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{multirow}
\usepackage[usenames, dvipsnames]{xcolor}
\definecolor{igual}{rgb}{0.21, 0.11, 0} 
\usepackage{hyperref}
\definecolor{dark-blue}{rgb}{0.0,0.0,0.1}
\definecolor{dark-green}{rgb}{0.0,0.1,0.0}
\definecolor{dark-red}{rgb}{0.1,0.0,0.0}
\hypersetup{
    colorlinks, linkcolor={dark-red},
    citecolor={dark-green}, urlcolor={dark-blue},
    pdftitle={What should Entity Linking link?},    % title
    pdfauthor={Henry Rosales-Méndez, Aidan Hogan and Barbara Poblete},     % author
    pdfsubject={LD4IE 2017},   % subject of the document
    pdfkeywords={multilingual;} {entity linking;} {information extraction;}, % list of keywords
}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

%para el simbolo de chequeado
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%
\newcommand{\xmark}{}%

\usepackage{moreverb}% usar verbatim + box
\usepackage{breqn}

\usepackage{booktabs}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{soul}
\usepackage{soul} %underline
\newcommand{\bl}[1]{\textbf{#1}}

\begin{document}

\title{Machine Translation vs.\\Multilingual Entity Linking}
%
%
\author{Henry Rosales-M\'endez, Aidan Hogan and Barbara Poblete}
%
\authorrunning{Rosales-Méndez et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
%\institute{Center for Semantic Web Research, DCC, University of Chile \\
%\texttt{\{hrosales,bpoblete,ahogan\}@dcc.uchile.cl}}
\institute{Millenium Institute for Foundational Research on Data\\Department of Computer Science, University of Chile 
%	\\\texttt{\{hrosales,ahogan,bpoblete\}@dcc.uchile.cl}
}

%-----------------------------------------
\maketitle              % typeset the title of the contribution
\begin{abstract}
Entity Linking (EL) associates the entities mentioned in a given input text with their corresponding knowledge-base (KB) entries. A recent EL trend is towards multilingual approaches. However, one may ask: are multilingual EL approaches necessary with recent advancements in machine translation? Could we not simply focus on supporting one language in the EL system and translate the input text to that language? We present experiments along these lines comparing multilingual EL systems with their results over machine translated text.
\end{abstract}


%---------------------
\section{Introduction}
\label{sec:intro}

Entity Linking (EL) associates the entities mentioned in a given input text with their corresponding knowledge-base (KB) identifiers; e.g., taking Wikidata as a target KB, for the input text ``\textit{Michael Jackson was born in Gary, Indiana}'', we can link \textit{Michael Jackson} with the Wikidata identifier \url{wd:Q2831}. However, multiple KB entities may have the same label; e.g., \url{wd:Q167877}, \url{wd:Q6831554}, and \url{wd:Q3856193} are all identifiers for people called \textit{Michael Jackson} in Wikidata. On the other hand, the same entity can be mentioned multiple ways, e.g., ``\textit{Michael J. Jackson}'', ``\textit{Jackson}'', ``\textit{King of Pop}'', etc., can refer to \url{wd:Q2831}.

Another practical challenge is being able to cope with input texts from various languages. While many EL approaches have been proposed down through the years, only recently have multilingual EL approaches -- configurable for various input languages -- become more popular (e.g.,~\cite{ferragina2010tagme,daiber2013improving,Babelfy-moro2014entity,freme-ner2016}). 
%In order to support multiple languages, an EL approach has three main options: (1) employ language-agnostic components that can work with respect to any KB that has labels in the required language~\cite{KIM-popov2004kim,daiber2013improving}; however, this approach precludes the possibility of using many linguistic components -- such as Part-Of-Speech (POS) taggers, dependency parsers, etc. -- that are tied to a language but can help improve EL quality; (2) include language-specific components that may improve EL quality, but for a variety of languages, such as POS taggers for various languages~\cite{Babelfy-moro2014entity}; (3) focus on supporting one language, but employ external machine translation approaches to translate the input text to the given language~\cite{Monahan2011,Cassidy2012}.
%
 Despite this trend, there are few studies evaluating multilingual EL. Hence, in our paper accepted for the Resource Track at ISWC~\cite{ourISWC}, we propose a multilingual EL benchmark and use it to perform experiments in order to study the behaviour of state-of-the-art multilingual approaches for five languages: English, French, German, Italian, and Spanish. We call our dataset VoxEL; a particular design goal of the dataset is to have (insofar as possible) the same text in different languages, and in particular, the same annotations per sentence across languages. Thus performance across languages -- not just systems -- can be compared directly. We also compare the results of multilingual EL systems with what would be possible using a state-of-the-art machine translation approach (Google Translate) to translate the text to English (the primary language supported by most tools). We refer the reader to our Resource Track paper~\cite{ourISWC} for more details.

In this poster, we present some additional results omitted from the full paper for reasons of space. More specifically, the poster focuses on the question of how an \textit{a priori} machine translation process compares with multilingual EL approaches, contributing novel results using VoxEL to evaluate EL performance using machine translation of the input to languages other than English. More generally, in the poster session, we would like to discuss with interested attendees the interplay between multilingual EL and machine translation; the results presented in this short paper provide important insights on this particular topic.


%-----------------------------------------
\section{Evaluating Multilingual Entity Linking Approaches}

%For a given text input with a set of entities mentioned $M$, and a KB with a set of identifiers $E$ and associated metadata, the EL task involves associating (or linking) each $m\in{}M$ with a corresponding entity $e\in{}E$. This EL process is commonly divided into two main phases: Entity Recognition (ER) and Entity Disambiguation (ED). The first one has the goal of identifying the entities mentions in the text. These identified entity mentions are passed to the second phase where each of them is disambiguated and linked with a KB identifier.
 
A multilingual EL system is characterised by being configurable for multiple input languages. In this work, we evaluate four multilingual EL systems with public APIs, namely \textit{Babelfy}~\cite{Babelfy-moro2014entity}, \textit{DBpedia Spotlight}~\cite{daiber2013improving}, \textit{FREME}~\cite{freme-ner2016} and \textit{TagME}~\cite{ferragina2010tagme}. For reasons of space, we refer to our previous work~\cite{ourLD4ID2017,ourISWC} for further details on these systems and other multilingual EL systems proposed in the literature.

%Details of the evaluated systems are presented in Table \ref{tab:multilingual_approaches}, where we present the year of publication, and the languages for which languages are evaluated in the publication (we note that the system may support more languages than evaluated). We observe that all systems support English, and that other languages evaluated are European in origin.


%\begin{table}[t!]
%	\centering
%	\caption{Overview of multilingual EL approaches evaluated in our experiments.}
%	\label{tab:multilingual_approaches}
%	\setlength{\tabcolsep}{3pt}
%	\begin{tabular}{lcccccr}
%		\toprule
%		\textbf{Name} &  \textbf{Year} & \textbf{Evaluated Languages} & \textbf{API}\\ \midrule
%		
%%		KIM \cite{KIM-popov2004kim} & 2004 &English, French, Spanish&\xmark&\cmark\\\midrule
%        
%        TagME \cite{ferragina2010tagme} & 2010 & English, German, Dutch & \cmark\\\midrule
%		
%%		SDA \cite{SDA-charton2011automatic}  & 2011 &English, French&\xmark&\xmark\\\midrule
%%        
%%        \textcolor{blue}{Monahan et al.}~\cite{Monahan2011} & 2011 & English, Chinese&\cmark & \xmark\\\midrule 
%%		
%%		ualberta \cite{guo2012ualberta}& 2012 &English, Chinese&\xmark&\xmark\\\midrule
%%		
%%		HITS \cite{fahrni2012hits} & 2012 & English, Spanish, Chinese&\xmark&\xmark\\\midrule
%%		
%%		THD \cite{THD-dojchinovski2012recognizing}  & 2012 &English, German, Dutch&\xmark&\cmark\\\midrule 
%%        
%%        \textcolor{blue}{Cassidy et al.}~\cite{Cassidy2012} & 2012  &English, Chinese&\cmark \xmark\\\midrule 
%		
%		\textit{DBpedia Spotlight}~\cite{daiber2013improving} & 2013  &English, Italian, Russian, Dutch, French,& \cmark\\
%		& &German, Spanish, Hungarian, Danish&&\\\midrule
%%		
%%		Wang-Tang \cite{wang2013boosting} & 2013 & English, Chinese&\xmark&\xmark\\\midrule
%%		
%%		MAG \cite{mag2017}& 2014 & English, German, Spanish&\xmark&\cmark\\
%%		& &French, Italian, Japanese,&&&\\
%%		& &Dutch&&&\\\midrule
%%		
%		\textit{Babelfy} \cite{Babelfy-moro2014entity}& 2014 &English, Spanish, French, German, Italian & \cmark\\\midrule
%%		
%%		WikiME \cite{Cross-Lingual-Wikifier-tsai2016cross} & 2016 & English, Spanish, French,&\xmark&\xmark\\
%%		& &Italian, Chinese, German,&&&\\
%%		& &Thai, Arabic, Turkish,&&&\\
%%		& &Tamil, Tagalog, Urdu, Hebrew&&&\\\midrule		
%%        
%        \textit{FREME}~\cite{freme-ner2016}&2016&English, German&\cmark\\
%		%& &French, Spanish, Russian&&&\\\midrule
%%		
%%		FEL~\cite{FEL-pappu2017lightweight}& 2017 &English, Spanish, Chinese&\xmark&\xmark\\
%%
%%		%&2016&IDIOMAS&\cmark&\cmark&\cmark\\
%		%& & French, Dutch &&&\\
%		\bottomrule
%	\end{tabular}
%\end{table}

%Instead of adding complexity to monolingual models in order to deal with a multilingual environment, others authors take advantage of machine translation to deal with several languages. For instance, Monahan et al.~\cite{Monahan2011} use Bing API\footnote{\url{http://www.microsofttranslator.com/dev/}; June 1st, 2018.} to translate the input text from Chinese to English, while Cassidy et al.~\cite{Cassidy2012} apply EL over Chinese using coreference resolution, various name translation techniques over the entity mentions to English, and machine translation to English. The use of machine translation brings advantages. In this way, monolingual systems deal with all the languages supported by the translator tool, which usually include the most popular languages in the world. Another advantage is that it is not necessary to deal with transliterations or cross-language links in cases when the target KB is available in a different language than the input text. 

%\section{VoxEL: A Multilingual EL Benchmark}


Evaluating multilingual EL systems requires benchmark datasets with texts in various languages. To further compare the quality of EL results across languages -- not just systems -- we need (insofar as possible) the same text and annotations in the different languages. Only a few such datasets have been proposed: TAC KBP\footnote{\url{https://tac.nist.gov/2017/KBP/}; June 1st, 2018.}, SemEval\footnote{\url{http://alt.qcri.org/semeval2018/}; June 1st, 2018.}, and MEANTIME~\cite{meantime16}. However, MEANTIME~\cite{meantime16} is the only publicly available dataset; SemEval is published by a third-party whereas the TAC KBP dataset we could not acquire. Furthermore, we found that these datasets exhibit differences in their annotations for different languages. For a more detailed explanation of multilingual benchmark datasets see~\cite{ourISWC} and for results comparing various EL systems over the SemEval dataset, see~\cite{ourLD4ID2017}.

To support multilingual EL evaluation, in~\cite{ourISWC} we proposed VoxEL: a curated text extracted from the multilingual VoxEurop news site\footnote{\url{http://www.voxeurop.eu}; June 1st, 2018.} and manually annotated for EL benchmarking. This dataset contains 15 documents for each of the five supported languages: Germany, English, Spanish, French and Italian. To support comparison across languages, VoxEL was edited to ensure the same annotations per sentence across languages, normalising variances across languages. Given a lack of consensus on the definition of ``entity'', VoxEL features two annotated version of the documents for each language: one \textit{strict} that includes entities referring to people, places and organisations, and \textit{relaxed} that includes links to all unambiguous pages of Wikipedia. Per language, VoxEL contains 237 and 635 annotations in the strict and relaxed version respectively.

%-----------------------------------------
\section{Experiments}

We conduct experiments using VoxEL to compare the behaviour of the four aforementioned multilingual EL systems for the five different languages offered by the dataset: German (DE), English (EN), Spanish (ES), French (FR) and Italian (IT). All systems were configured with their default parameters, except Babelfy, which allows to select a more strict or more relaxed notion of entity; we study the performance of both, denoted henceforth as Babelfy$_S$ and Babelfy$_R$ respectively. Aside from testing EL over the text in its native language, we also include results for EL applying machine translation -- namely Google Translate\footnote{\url{https://translate.google.com}; June 1st, 2018.} -- from each language of VoxEL to the other four languages; the purpose of this approach is to simulate an EL approach supporting one language and see if EL performs competitively when input text is translated from other languages. 

The results of the experiments are given in Table~\ref{tab:expriments}, where we present the $F_1$-measure for various configurations. On the left we present the system and language configured. At the top of the table we present the Relaxed and Strict versions of the dataset, where for each version, we present the language of the input text, which is machine translated to the configured language; for example, the row \textit {ES}, column \textit{DE$\rightarrow{}\_t$}, gives the result for a German input text translated to Spanish and processed by the given EL systems configured for Spanish. Where input and translated languages coincide, we use the input text directly (such results are indicated with boxes). The best result per row for each dataset version is presented in bold. Note that TagME supports English and German only.

\begin{table}[t!]
	\centering
	\caption{Comparison of EL systems for native and translated texts (F$_1$ measure)}
	\label{tab:expriments}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{@{}lcccccccccccc@{}}
			%\begin{tabular}{@{}lllllllllll@{}}
			\toprule
			&& \multicolumn{5}{c}{Relaxed} & \multicolumn{5}{c}{Strict}\\ \midrule
			&&DE$\rightarrow${}\_$_t$&EN$\rightarrow${}\_$_t$&ES$\rightarrow${}\_$_t$&FR$\rightarrow${}\_$_t$&IT$\rightarrow${}\_$_t$&DE$\rightarrow${}\_$_t$&EN$\rightarrow${}\_$_t$&ES$\rightarrow${}\_$_t$&FR$\rightarrow${}\_$_t$&IT$\rightarrow${}\_$_t$\\
			\cmidrule(lr){3-7}\cmidrule(lr){8-12}
			                &$DE$&~\fbox{\bf 0.523}~&~0.498~&~0.495~&~0.492~&~0.490~  &~\fbox{0.344}~&~0.342~&~0.365~&~{\bf 0.369}~&~0.367~\\
			                &$EN$&~0.508~&~\fbox{\bf 0.545}~&~0.515~&~0.506~&~0.502~  &~0.299~&~\fbox{\bf 0.319}~&~0.299~&~0.315~&~0.301~\\
            Babelfy$_R$~~~~~&$ES$&~0.525~&~{\bf 0.558}~&~\fbox{0.541}~&~0.552~&~0.548~  &~0.344~&~0.356~&~\fbox{\bf 0.362}~&~0.357~&~0.348~\\
                            &$FR$&~0.493~&~0.485~&~{\bf 0.502}~&~\fbox{0.493}~&~0.493~  &~0.332~&~0.331~&~{\bf 0.342}~&~\fbox{0.309}~&~0.341~\\
                            &$IT$&~0.513~&~0.527~&~0.512~&~{\bf 0.533}~&~\fbox{0.504}~  &~0.366~&~{\bf 0.379}~&~0.377~&~0.378~&~\fbox{0.365}~\\\midrule
                            &$DE$&~\fbox{0.279}~&~0.271~&~0.275~&~{\bf 0.285}~&~0.273~  &~\fbox{0.572}~&~0.584~&~0.589~&~{\bf 0.606}~&~0.588~\\
			                &$EN$&~0.312~&~\fbox{0.308}~&~0.309~&~{\bf 0.323}~&~0.304~  &~0.518~&~\fbox{\bf 0.567}~&~0.523~&~0.559~&~0.533~\\
            Babelfy$_S$~~~~~&$ES$&~0.318~&~0.327~&~\fbox{0.325}~&~0.334~&~{\bf 0.336}~  &~0.577~&~0.607~&~\fbox{\bf 0.611}~&~0.610~&~0.590~\\
                            &$FR$&~0.301~&~0.299~&~{\bf 0.312}~&~\fbox{0.290}~&~0.310~  &~0.574~&~0.601~&~{\bf 0.608}~&~\fbox{0.583}~&~0.606~\\
                            &$IT$&~0.306~&~0.319~&~0.318~&~{\bf 0.321}~&~\fbox{0.311}~  &~0.604~&~0.634~&~{\bf 0.640}~&~0.638~&~\fbox{0.616}~\\\midrule
                            &$DE$&~\fbox{\bf 0.400}~&~0.139~&~0.177~&~0.155~&~0.166~  &~\fbox{\bf 0.510}~&~0.220~&~0.292~&~0.248~&~0.280~\\
			                &$EN$&~0.442~&~\fbox{\bf 0.466}~&~0.454~&~0.465~&~0.450~  &~0.697~&~\fbox{0.707}~&~0.695~&~0.722~&~{\bf 0.730}~\\
            DBpedia Spotlight&$ES$&~0.159~&~0.121~&~\fbox{\bf 0.373}~&~0.130~&~0.199~  &~0.292~&~0.209~&~\fbox{\bf 0.513}~&~0.234~&~0.350~\\
                            &$FR$&~0.176~&~0.177~&~0.181~&~\fbox{\bf 0.314}~&~0.180~  &~0.245~&~0.252~&~0.252~&~\fbox{\bf 0.464}~&~0.255~\\
                            &$IT$&~0.184~&~0.163~&~0.221~&~0.158~&~\fbox{\bf 0.382}~  &~0.272~&~0.219~&~0.335~&~0.223~&~\fbox{\bf 0.601}~\\\midrule
                            &$DE$&~\fbox{\bf 0.282}~&~0.072~&~0.132~&~0.114~&~0.160~  &~\fbox{\bf 0.483}~&~0.154~&~0.240~&~0.179~&~0.261~\\
			                &$EN$&~0.401~&~\fbox{\bf 0.407}~&~0.402~&~0.397~&~0.406~  &~0.700~&~\fbox{0.708}~&~{\bf 0.715}~&~0.694~&~0.713~\\
            FREME    ~~~~~~ &$ES$&~0.174~&~0.117~&~\fbox{\bf 0.302}~&~0.147~&~0.232~  &~0.319~&~0.231~&~\fbox{\bf 0.583}~&~0.269~&~0.417~\\
                            &$FR$&~0.167~&~0.143~&~0.169~&~\fbox{\bf 0.268}~&~0.214~  &~0.287~&~0.278~&~0.314~&~\fbox{\bf 0.483}~&~0.322~\\
                            &$IT$&~0.164~&~0.127~&~0.205~&~0.136~&~\fbox{\bf 0.373}~  &~0.321~&~0.253~&~0.413~&~0.256~&~\fbox{\bf 0.726}~\\\midrule
           \multirow{2}{*}{TagME}                 &$DE$&~\fbox{\bf 0.414}~&~0.100~&~0.127~&~0.119~&~0.124~  &~\fbox{\bf 0.272}~&~0.122~&0.153&~0.137~&~0.152~\\
			                &$EN$&~0.432~&~\fbox{\bf 0.462}~&~0.450~&~0.442~&~0.440~  &~0.331~&~\fbox{0.327}~&~0.334~&~0.321~&~{\bf 0.336}~\\\bottomrule 
		\end{tabular}%
	}
\end{table}


\section{Discussion}

In Table~\ref{tab:expriments}, we would expect boxed (non-translated) and bold (best) results to correspond. While this is generally the case for DBpedia Spotlight and FREME; it is not the case for Babelfy, which (curiously) sometimes performs slightly better over machine translated texts in a given language; however, the margins here are slight. On the other hand, reading down the columns, for DBpedia Spotlight, FREME and TagME, these systems clearly perform better for non-English inputs when the text is machine translated to English rather than inputting the text directly and configuring the system for that language. This suggests that machine translation makes little difference in the case of Babelfy, but markedly improves the performance of other systems when dealing with non-English texts; the reasons for this may include the quality of language-specific components, the richness of KB information available for a particular language, etc. 

Furthermore, it is important to note in such cases that the output of the EL process after translation is still in the translated text; e.g., if we process text in French by translating it to English and performing EL configured for English, we get better results, but output a text in English rather than French. But we put forward that given (1) a high(er) quality annotation in the translated English text, (2) a sentence-to-sentence correspondence between the French and translated English text, and (3) cross-language links provided by KBs; it would not be difficult to ``transfer'' the annotations back to the original French text. 

In any case, these results raise the question of what role machine translation should play for EL, and indeed, in what circumstances it makes sense to develop multilingual EL systems, and in what circumstances it makes sense to develop monolingual EL systems with \textit{a priori} translation. We believe that such issues would be interesting to discuss in the context of the poster session.

%{\footnotesize
%\paragraph{Acknowledgements} The work of Henry Rosales-M\'endez was supported by CONICYT-PCHA/Doctorado Nacional/2016-21160017. The work was also supported by the Millennium Nucleus Center for Semantic Web Research under Grant NC120004.}


\bibliographystyle{splncs}
\begin{thebibliography}{99}

\bibitem{Cassidy2012}
Cassidy, T., Ji, H., Deng, H., Zheng, J., and Han, J. Analysis and refinement of cross-lingual entity linking. In CLEF (2012) 1--12

\bibitem{SDA-charton2011automatic}
Charton, E., Gagnon, M., Ozell, B.: Automatic semantic web annotation of named entities. In: Canadian Conference on Artificial Intelligence, Springer (2011) 74--85

\bibitem{WikiMe16}
Chen-Tse Tsai and Dan Roth. Cross-lingual wikification using multi-lingual embeddings. In Proceedings of NAACL-HLT, pages 589–598, 2016.

\bibitem{daiber2013improving}
Daiber, J., Jakob, M., Hokamp, C., Mendes, P.N.: Improving efficiency and accuracy in multilingual entity extraction. In: I-SEMANTICS, ACM (2013) 121--124

\bibitem{THD-dojchinovski2012recognizing}
Dojchinovski, M., Kliegr, T.: Recognizing, classifying and linking entities with {W}ikipedia and {DB}pedia. WIKT (2012) 41--44

\bibitem{fahrni2012hits}
Fahrni, A., G{\"o}ckel, T., Strube, M.: {HITS'} monolingual and cross-lingual entity linking system at {TAC} 2012: A joint approach. In: TAC, Citeseer (2012)

\bibitem{ferragina2010tagme} %10
Ferragina, P., Scaiella, U.: Tagme: on-the-fly annotation of short text fragments (by Wikipedia entities). In: CIKM, ACM (2010) 1625--1628

\bibitem{guo2012ualberta}
Guo, Z., Xu, Y., de S{\'a} Mesquita, F., Barbosa, D., Kondrak, G.: ualberta at {TAC-KBP} 2012: English and cross-lingual entity linking. In: TAC. (2012)

\bibitem{meantime16} %11
Minard, A. L., Speranza, M., Urizar, R., Altuna, B., van Erp, M. G. J., Schoen, A. M., van Son, C. M. MEANTIME, the NewsReader multilingual event and time corpus. LREC-ELRA (2016)

\bibitem{Babelfy14}
Moro, A., Raganato, A.,  Navigli, R. Entity linking meets word sense disambiguation: a unified approach. TACL \textbf{2} (2014) 231--244

\bibitem{Monahan2011}
Monahan, S., Lehmann, J., Nyberg, T., Plymale, J., and Jung, A. Cross-Lingual Cross-Document Coreference with Entity Linking. In TAC (2011)

\bibitem{mag2017}
Moussallem, D., Usbeck, R., R\"oeder, M., Ngomo, A. C. N. {MAG}: {A} Multilingual, Knowledge-base Agnostic and Deterministic Entity Linking Approach. {K-CAP} 2017 , ACM, (2017) 9:1--9:8

\bibitem{Babelfy-moro2014entity}
Moro, A., Raganato, A., Navigli, R.: Entity linking meets word sense disambiguation: a unified approach. Trans. of the ACL \textbf{2} (2014) 231--244

\bibitem{FEL-pappu2017lightweight}
Pappu, A., Blanco, R., Mehdad, Y., Stent, A., Thadani, K.: Lightweight multilingual entity extraction and linking. In: WSDM, ACM (2017) 365--374

\bibitem{KIM-popov2004kim}
Popov, B., Kiryakov, A., Ognyanoff, D., Manov, D., Kirilov, A.: KIM--a semantic platform for information extraction and retrieval. Natural Language Engineering \textbf{10}(3-4) (2004) 375--392

\bibitem{ourISWC}
Rosales-Méndez, H., Hogan A., Poblete B. VoxEL: A Benchmark Dataset for Multilingual Entity Linking. In ISWC (2018) (to appear)

\bibitem{ourLD4ID2017}
Rosales-Méndez, H., Poblete, B., and Hogan, A. Multilingual Entity Linking: Comparing English and Spanish. In LD4IE@ISWC (2017)

\bibitem{ourAMW2018}
Rosales-Méndez, H, Poblete, B., and Hogan, A. What should Entity Linking link? In AMW (2018)

\bibitem{freme-ner2016}
Sasaki, F., Dojchinovski, M., Nehring, J. Chainable and Extendable Knowledge Integration Web Services. In ISWC, (2016) 89--101

\bibitem{Cross-Lingual-Wikifier-tsai2016cross}
Tsai, C.T., Roth, D.: Cross-lingual wikification using multilingual embeddings. In: NAACL-HLT. (2016) 589--598

\bibitem{gerbil2015}
Usbeck, R. et al. GERBIL -- General Entity Annotation Benchmark Framework. In 24th WWW conference, 2015

\bibitem{wang2013boosting}
Wang, Z., Li, J., Tang, J.: Boosting cross-lingual knowledge linking via concept annotation. In: IJCAI. (2013) 2733--2739


\end{thebibliography}

\end{document}

